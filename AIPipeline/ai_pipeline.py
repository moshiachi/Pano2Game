import json
import os
import glob
import google.generativeai as genai
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import argparse
import re # Import the regular expressions library

# --- Configuration ---
# For security, it's best to set your API key as an environment variable.
# For example, in your terminal: export GEMINI_API_KEY="YOUR_API_KEY"
# The script will then automatically load it.
try:
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])
    print("[AI_PIPELINE] Gemini API key configured successfully from environment variable.")
except KeyError:
    print("CRITICAL ERROR: The 'GEMINI_API_KEY' environment variable is not set.")
    print("Please set it before running the script. For example:")
    print("export GEMINI_API_KEY='YOUR_API_KEY_HERE'")
    exit()


# --- Paths ---
# Path to your Unity project's Assets folder
UNITY_ASSETS_PATH = '../UnityGaussianSplatting/projects/GaussianExample/Assets'
# Path where the C# VLM analysis files are saved
VLM_OUTPUT_PATH = os.path.join(UNITY_ASSETS_PATH, 'SceneCaptures_bicycle/VLM_Analysis_Outputs')
# Path to your asset catalog
ASSET_CATALOG_PATH = os.path.join(UNITY_ASSETS_PATH, 'StreamingAssets/AssetCatalog_processed.json')
# Path to the pre-computed asset embeddings
ASSET_EMBEDDINGS_PATH = 'asset_embeddings.json'
# Path to the prompt for the final C# code generation
CODE_GEN_PROMPT_PATH = 'prompts/code_generation_prompt.txt'
# Path to the new prompt for AI-driven localization
LOCALIZATION_PROMPT_PATH = 'prompts/localization_prompt.txt'
# Path to the new prompt for AI-driven consolidation
CONSOLIDATION_PROMPT_PATH = 'prompts/consolidation_prompt.txt'
# The Gemini model to use for C# code generation
CODE_GEN_MODEL = 'gemini-2.5-flash-preview-05-20'
# The Gemini model to use for AI tasks
AI_MODEL = 'gemini-2.5-flash-preview-05-20'
# The model for generating text embeddings for RAG
EMBEDDING_MODEL = 'text-embedding-004'


def load_asset_catalog():
    """Loads the manually curated asset catalog from the Unity project."""
    try:
        with open(ASSET_CATALOG_PATH, 'r') as f:
            print(f"[AI_PIPELINE] Successfully loaded asset catalog from: {ASSET_CATALOG_PATH}")
            return json.load(f)
    except Exception as e:
        print(f"Error loading asset catalog at '{ASSET_CATALOG_PATH}': {e}")
        return None

def load_asset_embeddings():
    """Loads the pre-computed asset embeddings."""
    try:
        with open(ASSET_EMBEDDINGS_PATH, 'r') as f:
            embeddings_data = json.load(f)
            print(f"[AI_PIPELINE] Successfully loaded {len(embeddings_data)} asset embeddings.")
            return embeddings_data
    except FileNotFoundError:
        print(f"CRITICAL ERROR: Asset embeddings file not found at '{ASSET_EMBEDDINGS_PATH}'.")
        print("Please run 'python generate_asset_embeddings.py' first to create it.")
        return None
    except Exception as e:
        print(f"Error loading asset embeddings file: {e}")
        return None

def load_vlm_analysis_files(indices=None):
    """
    Loads VLM analysis JSON files generated by Unity.
    Can be filtered by a list of integer indices provided via command line.
    """
    # This function no longer uses the test_file logic directly
    all_files = glob.glob(os.path.join(VLM_OUTPUT_PATH, '*_vlm_scene.json'))
    
    # --- New Natural Sort Logic ---
    # Define a key for sorting that extracts the number from the filename
    def natural_sort_key(filename):
        # Find all numbers in the string
        numbers = re.findall(r'\d+', filename)
        # Return the first number found, or 0 if none
        return int(numbers[0]) if numbers else 0

    # Sort the files using the natural sort key
    json_files = sorted(all_files, key=natural_sort_key)
    
    if not json_files:
        print(f"Warning: No VLM analysis files (*_vlm_scene.json) found in '{VLM_OUTPUT_PATH}'.")
        return []

    if indices:
        print(f"--- MANUAL SELECTION: Attempting to load files at indices: {indices} ---")
        selected_files = []
        for i in indices:
            try:
                selected_files.append(json_files[i])
            except IndexError:
                print(f"Warning: Index {i} is out of range. There are only {len(json_files)} files. Skipping this index.")
        
        if not selected_files:
            print("No valid files were selected with the given indices.")
            return []
        
        print("Selected files:")
        for f in selected_files:
            print(f"  - {os.path.basename(f)}")
        return selected_files
    
    print(f"--- PROCESSING ALL {len(json_files)} VLM-generated files. ---")
    return json_files

def consolidate_objects_with_ai(all_objects):
    """
    Uses an LLM to de-duplicate a list of objects detected from multiple views.
    """
    print(f"\n[Consolidation] Starting AI-driven object consolidation for {len(all_objects)} detected objects...")
    if len(all_objects) <= 1:
        print("[Consolidation] Not enough objects to require consolidation. Skipping.")
        return all_objects
        
    try:
        with open(CONSOLIDATION_PROMPT_PATH, 'r') as f:
            prompt_template = f.read()
    except Exception as e:
        print(f"Error loading consolidation prompt from '{CONSOLIDATION_PROMPT_PATH}': {e}")
        return all_objects # Return original list on error

    # Prepare the JSON data for the prompt
    detected_objects_json = json.dumps(all_objects, indent=2)
    prompt = prompt_template.format(detected_objects_json=detected_objects_json)

    print("[Consolidation] Contacting Gemini to merge duplicate objects...")
    try:
        model = genai.GenerativeModel(AI_MODEL)
        response = model.generate_content(prompt)
        
        json_text = response.text.strip()
        if "```json" in json_text:
            json_text = json_text.split("```json")[1].split("```")[0]
            
        consolidated_data = json.loads(json_text)
        final_list = consolidated_data.get("consolidated_objects", [])
        
        print(f"[Consolidation] Consolidation complete. Reduced {len(all_objects)} detections to {len(final_list)} unique objects.")
        return final_list
        
    except Exception as e:
        print(f"[Consolidation] An error occurred during AI-driven consolidation: {e}")
        print("[Consolidation] Proceeding with the original, un-consolidated list.")
        return all_objects

def perform_ai_rag_matching(vlm_details, asset_embeddings, asset_catalog):
    """
    Performs AI-powered RAG using semantic search over pre-computed asset embeddings.
    """
    object_name = vlm_details.get("identified_object_name", "").lower()
    description = vlm_details.get("general_description", "")
    query_text = f"Object: {object_name}. Description: {description}"
    
    print(f"[AI_RAG] Performing semantic search for: '{object_name}'")

    try:
        # 1. Get the embedding for the VLM's description of the object
        query_embedding_result = genai.embed_content(
            model=EMBEDDING_MODEL,
            content=query_text,
            task_type="RETRIEVAL_QUERY"
        )
        query_vector = np.array(query_embedding_result['embedding']).reshape(1, -1)

        # 2. Compare with all pre-computed asset embeddings
        best_match = None
        highest_similarity = -1

        for asset_data in asset_embeddings:
            asset_vector = np.array(asset_data['embedding']).reshape(1, -1)
            
            # Calculate cosine similarity
            similarity = cosine_similarity(query_vector, asset_vector)[0][0]
            
            if similarity > highest_similarity:
                highest_similarity = similarity
                best_match = asset_data['asset_info']

        if best_match and highest_similarity > 0.6: # Using a threshold to avoid poor matches. 0.5 -> 0.6
            print(f"---> Found best semantic match: '{best_match['asset_name_id']}' with similarity score {highest_similarity:.4f}")
            
            best_material_path = None
            # Only search for a material if the matched prefab is generic
            if best_match.get("is_generic_asset", False):
                print(f"---> Match is generic. Searching for override material...")
                best_material_path = find_best_material_for_object(vlm_details, asset_catalog)
            else:
                print(f"---> Match is not generic. Skipping material search.")

            return {
                "asset_path": best_match['path'],
                "material_path_override": best_material_path,
                "is_generic_asset": best_match.get("is_generic_asset", False),
                "default_collider_type": best_match.get('default_collider_type', 'box'),
                "estimated_mass_kg": best_match.get('estimated_mass_kg', 1)
            }

    except Exception as e:
        print(f"[AI_RAG] Error during semantic search: {e}")
    
    # --- FALLBACK LOGIC --- (If AI RAG fails or finds no good match)
    print(f"---> Semantic search failed or no good match found. Falling back to primitive generation for '{object_name}'.")
    return create_primitive_fallback(vlm_details, asset_catalog)

def find_best_material_for_object(vlm_details, asset_catalog):
    """Finds the best matching material from the catalog for a given object description."""
    object_name = vlm_details.get("identified_object_name", "").lower()
    materials = vlm_details.get("suggested_materials", [])
    best_material_path = None
    best_material_score = 0

    for asset in asset_catalog:
        if asset.get('type') != 'Material':
            continue
        
        material_score = 0
        asset_tags = [tag.lower() for tag in asset.get("tags", [])]

        if any(tag in object_name for tag in asset_tags): material_score += 5
        for material in materials:
            if any(material.lower() in tag for tag in asset_tags): material_score += 10
        
        if material_score > best_material_score:
            best_material_score = material_score
            best_material_path = asset.get('path')
    
    if best_material_path:
        print(f"---> Found matching material for fallback/override: '{best_material_path}'")
    return best_material_path

def create_primitive_fallback(vlm_details, asset_catalog):
    """Creates a recipe for a primitive object when no prefab match is found."""
    suggested_shape = vlm_details.get("collider_suggestion", {}).get("suggested_shape", "box")
    primitive_type = "box"
    if suggested_shape in ["box", "sphere", "capsule", "cylinder"]:
        primitive_type = suggested_shape

    best_material_path = find_best_material_for_object(vlm_details, asset_catalog)

    return {
        "asset_path": f"primitive::{primitive_type}",
        "material_path_override": best_material_path,
        "is_generic_asset": True, # Primitives are always generic and need a material
        "default_collider_type": primitive_type,
        "estimated_mass_kg": vlm_details.get("rigidbody_suggestion", {}).get("estimated_mass_kg", 50)
    }

def get_ai_driven_localization(matched_objects, scene_bounds):
    """
    Uses an LLM to determine plausible 3D positions and rotations for a list of objects
    based on their text descriptions and a central landmark.
    """
    if not matched_objects:
        print("[Localization] No matched objects to localize.")
        return {}

    # 1. Select a landmark. Heuristic: pick a common, central-sounding object, or the first one.
    landmark = None
    # Prefer more descriptive, central landmarks first
    landmark_candidates = ["central", "main", "lawn", "flowerbed", "path", "ground cover", "courtyard"]
    for obj in matched_objects:
        # Check both name and description for landmark keywords
        obj_name_desc = f"{obj['vlm_details']['identified_object_name']} {obj['vlm_details']['general_description']}".lower()
        if any(candidate in obj_name_desc for candidate in landmark_candidates):
            landmark = obj
            break
    if not landmark:
        landmark = matched_objects[0]
    
    landmark_name = landmark['vlm_details']['identified_object_name'].lower()
    print(f"[Localization] Selected landmark: '{landmark_name}'. It will be placed at origin (0,0,0).")

    # 2. Prepare the object descriptions for the prompt
    object_descriptions = []
    for obj in matched_objects:
        # We don't need to send all details, just the name and description
        object_descriptions.append({
            "name": obj['vlm_details']['identified_object_name'],
            "description": obj['vlm_details']['general_description']
        })

    # 3. Load the localization prompt
    try:
        with open(LOCALIZATION_PROMPT_PATH, 'r') as f:
            prompt_template = f.read()
    except Exception as e:
        print(f"Error loading localization prompt from '{LOCALIZATION_PROMPT_PATH}': {e}")
        return {}

    # 4. Populate the prompt with our scene context
    # Correctly format the JSON string to be embedded in the prompt
    descriptions_json_string = json.dumps(object_descriptions, indent=2)
    prompt = prompt_template.format(
        scene_width=scene_bounds.get('width', 50), 
        scene_depth=scene_bounds.get('depth', 50),
        landmark_name=landmark_name,
        object_descriptions_json=descriptions_json_string
    )

    # 5. Call the Gemini API for localization
    print("[Localization] Contacting Gemini for scene layout assistance...")
    try:
        model = genai.GenerativeModel(AI_MODEL)
        # Configure for JSON output if supported, otherwise parse from text
        response = model.generate_content(prompt)

        # Clean the response to get only the JSON part
        json_text = response.text.strip()
        if "```json" in json_text:
            json_text = json_text.split("```json")[1].split("```")[0]
        
        placements_data = json.loads(json_text)
        
        # 6. Process the response into a more useful dictionary
        final_placements = {}
        for item in placements_data.get("object_placements", []):
            obj_name = item.get("object_name")
            pos = item.get("position")
            rot_y = item.get("rotation_y")
            if obj_name and pos and rot_y is not None:
                final_placements[obj_name] = {
                    "position": {"x": pos.get('x', 0), "y": pos.get('y', 0), "z": pos.get('z', 0)},
                    "rotation_euler": {"x": 0, "y": rot_y, "z": 0},
                    "scale": {"x": 1.0, "y": 1.0, "z": 1.0} # Default scale for now
                }
        
        # Ensure the landmark itself is in the placement dictionary at the origin
        if landmark_name not in final_placements:
            print(f"[Localization] Landmark '{landmark_name}' not in AI response, adding it at origin.")
            final_placements[landmark_name] = {
                "position": {"x": 0, "y": 0, "z": 0},
                "rotation_euler": {"x": 0, "y": 0, "z": 0},
                "scale": {"x": 1.0, "y": 1.0, "z": 1.0}
            }

        print(f"[Localization] Successfully received and parsed {len(final_placements)} object placements from AI.")
        return final_placements

    except Exception as e:
        print(f"[Localization] An error occurred during AI-driven localization: {e}")
        return {}

def generate_scene_construction_script(object_list_for_llm):
    """Generates the C# scene construction script using the Gemini API."""
    print(f"[AI_PIPELINE] Contacting Gemini to generate C# script for {len(object_list_for_llm)} objects.")
    
    try:
        with open(CODE_GEN_PROMPT_PATH, 'r') as f:
            prompt_template = f.read()
    except Exception as e:
        print(f"Error loading C# code generation prompt from '{CODE_GEN_PROMPT_PATH}': {e}")
        return None

    # Convert the list of objects into a nicely formatted JSON string,
    # and escape it to be a C# verbatim string literal.
    csharp_json_string = json.dumps(object_list_for_llm, indent=4).replace('"', '""')

    # Replace the placeholder in the prompt with our actual JSON data
    # The new prompt expects the JSON to be part of the C# code block.
    final_prompt = prompt_template.replace("[PASTE_JSON_OBJECT_LIST_HERE]", csharp_json_string)

    try:
        # Initialize the Gemini client and generate the content
        model = genai.GenerativeModel(CODE_GEN_MODEL)
        response = model.generate_content(final_prompt)
        
        # Return the generated text, which should be the C# code
        print("[AI_PIPELINE] Successfully received C# code from Gemini.")
        # The LLM is now generating the full file, so we don't need to strip anything.
        # We just clean up potential markdown formatting.
        generated_text = response.text.strip()
        if generated_text.startswith("```csharp"):
            generated_text = generated_text[len("```csharp"):].strip()
        if generated_text.endswith("```"):
            generated_text = generated_text[:-len("```")].strip()
            
        return generated_text

    except Exception as e:
        print(f"An error occurred while calling the Gemini API: {e}")
        return None


def main_pipeline(args):
    """Main AI processing pipeline."""
    asset_embeddings = load_asset_embeddings()
    if not asset_embeddings:
        print("Aborting pipeline: Asset embeddings are required.")
        return

    asset_catalog = load_asset_catalog()
    if not asset_catalog:
        print("Aborting pipeline: Asset Catalog not found or empty.")
        return

    # Use the new indices argument to load files
    vlm_file_paths = load_vlm_analysis_files(indices=args.indices)
    if not vlm_file_paths:
        print("Aborting pipeline: No VLM analysis files found or selected.")
        return

    # 2. Read all files and aggregate every detected object
    all_vlm_objects = []
    print(f"Found {len(vlm_file_paths)} VLM analysis files to process.")
    for file_path in vlm_file_paths:
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
                if "scene_objects" in data and data["scene_objects"]:
                    for obj in data["scene_objects"]:
                        if obj.get("processingFailureReason") is None and obj.get("identified_object_name"):
                            all_vlm_objects.append(obj)
        except Exception as e:
            print(f"Error reading or parsing JSON file {file_path}: {e}")

    if not all_vlm_objects:
        print("Aborting pipeline: Could not extract any valid objects from the analysis files.")
        return

    # 3. Consolidate the aggregated list with AI
    consolidated_objects = consolidate_objects_with_ai(all_vlm_objects)
    if not consolidated_objects:
        print("Aborting pipeline: Consolidation resulted in an empty list of objects.")
        return

    # 4. Perform RAG and Localization on the clean, consolidated list
    matched_objects = []
    for vlm_details in consolidated_objects:
        rag_match = perform_ai_rag_matching(vlm_details, asset_embeddings, asset_catalog)
        if not rag_match:
            object_name_for_skip = vlm_details.get("identified_object_name", "unknown")
            print(f"Could not produce a match or fallback for '{object_name_for_skip}'. Skipping.")
            continue
        matched_objects.append({'vlm_details': vlm_details, 'rag_match': rag_match})
    
    if not matched_objects:
        print("No objects were successfully matched to the asset catalog. Aborting.")
        return

    scene_bounds = {'width': 50, 'depth': 50} 
    object_placements = get_ai_driven_localization(matched_objects, scene_bounds)

    if not object_placements:
        print("AI-driven localization failed. Cannot proceed to generate C# script.")
        return
        
    # ... The rest of the main_pipeline function remains the same ...
    # It will now build the final list for the C# generator from the clean data.
    all_objects_for_llm = []
    for obj_data in matched_objects:
        vlm_details = obj_data['vlm_details']
        rag_match = obj_data['rag_match']
        object_name = vlm_details.get("identified_object_name", "unknown_object").lower()

        placement = None
        # First, try an exact match (case-insensitive)
        for key, value in object_placements.items():
            if key.lower() == object_name.lower():
                placement = value
                break
        
        # If no exact match, try a partial match
        if not placement:
            for key, value in object_placements.items():
                if object_name.lower() in key.lower() or key.lower() in object_name.lower():
                    placement = value
                    print(f"-> Found fuzzy placement match for '{object_name}' with key '{key}'")
                    break

        # If placement is not found, check if it's a foundational object that the AI might have skipped.
        if not placement:
            foundational_keywords = ["ground", "lawn", "field", "floor", "terrain"]
            if any(keyword in object_name for keyword in foundational_keywords):
                print(f"-> Placement not found for foundational object '{object_name}'. Assuming origin (0,0,0).")
                placement = {
                    "position": {"x": 0, "y": 0, "z": 0},
                    "rotation_euler": {"x": 0, "y": 0, "z": 0},
                    "scale": {"x": 1.0, "y": 1.0, "z": 1.0}
                }

        if not placement:
            print(f"Warning: No placement information found for '{object_name}'. It will not be included in the final scene.")
            continue

        object_for_llm = {
            "asset_path": rag_match['asset_path'],
            "object_name_in_hierarchy": f"{object_name.replace(' ', '_')}_{len(all_objects_for_llm)}",
            "position": placement['position'],
            "rotation_euler": placement['rotation_euler'],
            "scale": placement['scale'],
            "material_path_override": rag_match.get("material_path_override"),
            "is_generic_asset": rag_match.get("is_generic_asset", False),
            "collider_info": {
                "type": vlm_details.get("collider_suggestion", {}).get("suggested_shape", rag_match.get("default_collider_type")),
                "is_trigger": False,
                "center": {"x":0,"y":0,"z":0}, "size": {"x":1,"y":1,"z":1}, "radius": 0.5, "height": 2.0, "direction": 1
            },
            "rigidbody_info": {
                "mass": vlm_details.get("rigidbody_suggestion", {}).get("estimated_mass_kg", rag_match.get("estimated_mass_kg")),
                "use_gravity": True,
                "is_kinematic": not vlm_details.get("rigidbody_suggestion", {}).get("is_likely_dynamic", False),
                "drag": 0.1, "angular_drag": 0.05
            }
        }
        all_objects_for_llm.append(object_for_llm)
    
    if not all_objects_for_llm:
        print("No objects were successfully matched to the asset catalog. Cannot generate script.")
        return

    print(f"\nFound {len(all_objects_for_llm)} unique objects to place. Preparing to generate C# script...")
    generated_script_body = generate_scene_construction_script(all_objects_for_llm)
    
    if generated_script_body:
        output_script_path = "GeneratedUnityScript.cs.txt"
        with open(output_script_path, 'w') as f:
            f.write(generated_script_body)
        print(f"\nSuccessfully generated C# script content at: {os.path.abspath(output_script_path)}")
        print("You can now use the Unity menu 'AI Tools > Update Scene Builder Script' to load this file.")
    else:
        print("Failed to generate C# script from Gemini.")

if __name__ == '__main__':
    # --- Command-Line Argument Parsing ---
    parser = argparse.ArgumentParser(description="Run the AI scene generation pipeline.")
    parser.add_argument(
        '--indices', 
        nargs='*', 
        type=int, 
        help="Optional: A space-separated list of integer indices for the VLM files to process (e.g., 0 2 5)."
    )
    args = parser.parse_args()

    main_pipeline(args) 